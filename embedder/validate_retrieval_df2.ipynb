{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c1b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/vnayok/pl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import typing as tp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from deepfashion import read_splits as read_deepfashion_splits, Crop\n",
    "from load_model import load_model\n",
    "from utils import apk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0eaea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Query:\n",
    "    crop: Crop\n",
    "    item_id: int\n",
    "    embedding: np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class GalleryItem:\n",
    "    crop: Crop\n",
    "    item_id: int\n",
    "    embedding: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432f362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(same_category_apks, same_item_apks, same_style_apks, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"same_category\": {k: np.mean(val) for k, val in same_category_apks.items()},\n",
    "                \"same_item\": {k: np.mean(val) for k, val in same_item_apks.items()},\n",
    "                \"same_style\": {k: np.mean(val) for k, val in same_style_apks.items()},\n",
    "            },\n",
    "            f,\n",
    "            indent=4,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de77deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP_FASHION_DIR = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c25ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"convnextv2_base\"\n",
    "MODEL_CHECKPOINT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230aa891",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7909cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, transform = load_model(MODEL_NAME, MODEL_CHECKPOINT, is_wrapped_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd5dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3159376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading validation split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 550/32153 [00:00<00:21, 1440.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32153/32153 [00:19<00:00, 1659.35it/s]\n"
     ]
    }
   ],
   "source": [
    "items_data = read_deepfashion_splits(DEEP_FASHION_DIR, [\"validation\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d605f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ead534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffa4fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 219/2279 [01:01<09:02,  3.79it/s]/mnt/data/vnayok/pl/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 2279/2279 [07:31<00:00,  5.05it/s]\n"
     ]
    }
   ],
   "source": [
    "querries = []\n",
    "gallery = []\n",
    "\n",
    "for item_id, crops in tqdm.tqdm(items_data.items()):\n",
    "    transformed = []\n",
    "    for crop in crops:\n",
    "        img = Image.open(crop.crop_file)\n",
    "        transformed.append(transform(img))\n",
    "    transformed = torch.stack(transformed).squeeze(1).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeds = model(transformed).cpu().numpy()\n",
    "    for i, crop in enumerate(crops):\n",
    "        if crop.source == \"user\":\n",
    "            querries.append(\n",
    "                Query(\n",
    "                    crop,\n",
    "                    item_id,\n",
    "                    embeds[i,:],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            gallery.append(\n",
    "                GalleryItem(\n",
    "                    crop,\n",
    "                    item_id,\n",
    "                    embeds[i,:],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea3cffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_embeds = np.stack([g.embedding for g in gallery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3b26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_counts = defaultdict(int)\n",
    "for g in gallery:\n",
    "    gallery_counts[g.item_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28bac2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10844/10844 [30:55<00:00,  5.84it/s]\n"
     ]
    }
   ],
   "source": [
    "same_category_apks = defaultdict(list)\n",
    "same_item_apks = defaultdict(list)\n",
    "same_style_apks = defaultdict(list)\n",
    "for q in tqdm.tqdm(querries):\n",
    "    csim = cosine_similarity(q.embedding[None,:], gallery_embeds)\n",
    "    gallery_ids = np.argsort(-csim)[0]\n",
    "    is_relevant = [(gallery[gid].crop.category_id == q.crop.category_id) for gid in gallery_ids]\n",
    "    for k in ks:\n",
    "        same_category_apks[k].append(apk(is_relevant, k))\n",
    "    is_relevant = [(gallery[gid].item_id == q.item_id) for gid in gallery_ids]\n",
    "    for k in ks:\n",
    "        same_item_apks[k].append(apk(is_relevant, k))\n",
    "    is_relevant = [(gallery[gid].item_id == q.item_id) and\n",
    "                   (gallery[gid].crop.item_style == q.crop.item_style)\n",
    "                   for gid in gallery_ids]\n",
    "    for k in ks:\n",
    "        same_style_apks[k].append(apk(is_relevant, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f0c4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(same_category_apks, same_item_apks, same_style_apks, \"validation_results/res.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
